{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9VYny0d-d-4"
      },
      "source": [
        "# بسم الله الرحمن الرحيم"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd6vQiU2-7zR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import zipfile\n",
        "devic = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfYdfp9WBkG6"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Data set/tennis_court_det_dataset.zip' , 'r') as f :\n",
        "  f.extractall('/content/TennisData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL6xPtRSDRV5"
      },
      "outputs": [],
      "source": [
        "class KeypointsDataset(Dataset):\n",
        "    def __init__(self, img_dir, data_file):\n",
        "        self.img_dir = img_dir\n",
        "        with open(data_file, \"r\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
        "        h,w = img.shape[:2]\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = self.transforms(img)\n",
        "        kps = np.array(item['kps']).flatten()\n",
        "        kps = kps.astype(np.float32)\n",
        "\n",
        "        kps[::2] *= 224.0 / w # Adjust x coordinates\n",
        "        kps[1::2] *= 224.0 / h # Adjust y coordinates\n",
        "\n",
        "        return img, kps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb3FLElnFhK5"
      },
      "outputs": [],
      "source": [
        "train_dataset = KeypointsDataset(\"/content/TennisData/data/images\",\"/content/TennisData/data/data_train.json\")\n",
        "val_dataset = KeypointsDataset(\"/content/TennisData/data/images\",\"/content/TennisData/data/data_val.json\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RTwk4acFzPt",
        "outputId": "98cadd6f-3788-44c5-b37c-c65e7fe3b99b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 129MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.fc =  torch.nn.Linear(model.fc.in_features, 14*2) # Replaces the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ij5j1y8GLbE"
      },
      "outputs": [],
      "source": [
        "model = model.to(devic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEmNL-FEGSik"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp-sn4nWGkaF",
        "outputId": "33d5306b-0acf-42e9-c7cd-5505b3f09ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, iter 828, loss: 204.71182250976562\n",
            "Epoch 1, iter 828, loss: 18.124347686767578\n",
            "Epoch 2, iter 828, loss: 16.66643714904785\n",
            "Epoch 3, iter 828, loss: 9.161162376403809\n",
            "Epoch 4, iter 828, loss: 52.74058532714844\n",
            "Epoch 5, iter 828, loss: 16.72909927368164\n",
            "Epoch 6, iter 828, loss: 2.0652616024017334\n",
            "Epoch 7, iter 828, loss: 4.2975568771362305\n",
            "Epoch 8, iter 828, loss: 1.46559476852417\n",
            "Epoch 9, iter 828, loss: 1.019932746887207\n",
            "Epoch 10, iter 828, loss: 1.3354132175445557\n",
            "Epoch 11, iter 828, loss: 0.929623007774353\n",
            "Epoch 12, iter 828, loss: 1.7763521671295166\n",
            "Epoch 13, iter 828, loss: 5.046415328979492\n",
            "Epoch 14, iter 828, loss: 0.8368075489997864\n",
            "Epoch 15, iter 828, loss: 1.9738987684249878\n",
            "Epoch 16, iter 828, loss: 2.9031198024749756\n",
            "Epoch 17, iter 828, loss: 2.8941245079040527\n",
            "Epoch 18, iter 828, loss: 2.9193115234375\n",
            "Epoch 19, iter 828, loss: 1.4006022214889526\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs,kps) in enumerate(train_loader):\n",
        "        imgs = imgs.to(devic)\n",
        "        kps = kps.to(devic)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, kps)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcD8bwntZZOH",
        "outputId": "190e231a-d656-45c9-8934-3aff40189132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.555735425183059\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "total_loss = 0.0\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for imgs, kps in val_loader:\n",
        "        imgs = imgs.to(devic)\n",
        "        kps = kps.to(devic)\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, kps)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "avg_loss = total_loss / len(val_loader)\n",
        "print(f\"Validation Loss: {avg_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JlX9x54aJa_",
        "outputId": "78744a7a-47c4-4975-afd5-5eb1d4085c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), 'tennis_keypoint_model.pth')\n",
        "print(\"Model saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
